{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033f569c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA AVAILABLE: False\n",
      "USING DEVICE: cpu\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW  # IMPORT ADAMW FROM TORCH.OPTIM INSTEAD OF TRANSFORMERS\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# KAGGLE SPECIFIC CONFIGURATION\n",
    "BASE_DIR = '/kaggle/input/ai-2-dl-for-nlp-2025-homework-3'\n",
    "TRAIN_CSV_PATH = os.path.join(BASE_DIR, 'train_dataset.csv')\n",
    "VAL_CSV_PATH = os.path.join(BASE_DIR, 'val_dataset.csv')\n",
    "TEST_CSV_PATH = os.path.join(BASE_DIR, 'test_dataset.csv')\n",
    "\n",
    "# OUTPUT DIRECTORY FOR SAVING MODELS AND SUBMISSIONS (KAGGLE'S WRITABLE DIRECTORY)\n",
    "OUTPUT_DIR = '/kaggle/working'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "# MODEL AND TRAINING HYPERPARAMETERS \n",
    "BATCH_SIZE = 64      # OPTIMIZED FOR KAGGLE T4 GPU [GOOGLE]\n",
    "MAX_LENGTH = 128     # MAX SEQUENCE LENGTH FOR TOKENIZATION\n",
    "BERT_LEARNING_RATE = 2e-5  # TYPICAL LEARNING RATE FOR BERT FINE-TUNING\n",
    "BERT_EPOCHS = 4      # NUMBER OF EPOCHS FOR FINE-TUNING\n",
    "WARMUP_STEPS = 0     # NO WARMUP STEPS\n",
    "\n",
    "# KAGGLE GPU OPTIMIZATION\n",
    "print(\"=== CHECKING KAGGLE GPU CONFIGURATION ===\")\n",
    "if torch.cuda.is_available():\n",
    "    # CONFIGURE FOR KAGGLE T4 GPU\n",
    "    print(\"KAGGLE T4 GPU DETECTED!\")\n",
    "    # SET OPTIMAL BATCH SIZE FOR T4 (16GB VRAM)\n",
    "    # USUALLY 64 WORKS WELL FOR BERT ON T4\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9  # GB\n",
    "    print(f\"GPU MEMORY: {gpu_mem:.1f} GB\")\n",
    "    \n",
    "    # ENABLE MIXED PRECISION TRAINING FOR FASTER PERFORMANCE\n",
    "    try:\n",
    "        from torch.cuda.amp import autocast\n",
    "        from torch.amp import GradScaler\n",
    "        scaler = GradScaler('cuda')\n",
    "        mixed_precision_available = True\n",
    "        print(\"MIXED PRECISION TRAINING ENABLED (FASTER TRAINING)\")\n",
    "    except:\n",
    "        mixed_precision_available = False\n",
    "        print(\"MIXED PRECISION TRAINING NOT AVAILABLE\")\n",
    "    \n",
    "    # SET CUDNN BENCHMARK FOR OPTIMAL PERFORMANCE\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"CUDNN BENCHMARK ENABLED FOR OPTIMAL PERFORMANCE\")\n",
    "    \n",
    "    # PRINT GPU DETAILS\n",
    "    print(f\"CUDA VERSION: {torch.version.cuda}\")\n",
    "    print(f\"GPU NAME: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"WARNING: GPU NOT DETECTED! TRAINING WILL BE SLOW.\")\n",
    "    mixed_precision_available = False\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"USING DEVICE: {DEVICE}\")\n",
    "\n",
    "# SET RANDOM SEEDS FOR REPRODUCIBILITY\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988cd718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFY DATASET PATHS FOR KAGGLE\n",
    "print(\"CHECKING KAGGLE DATASET PATHS...\")\n",
    "print(f\"TRAIN DATASET: {os.path.exists(TRAIN_CSV_PATH)}\")\n",
    "print(f\"VALIDATION DATASET: {os.path.exists(VAL_CSV_PATH)}\")\n",
    "print(f\"TEST DATASET: {os.path.exists(TEST_CSV_PATH)}\")\n",
    "\n",
    "# GPU MEMORY MANAGEMENT FOR KAGGLE T4 GPU\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"CLEAR GPU MEMORY CACHE TO FREE UP RESOURCES.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"GPU MEMORY CACHE CLEARED\")\n",
    "        # PRINT CURRENT MEMORY USAGE\n",
    "        print(f\"GPU MEMORY: ALLOCATED: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB, \"\n",
    "              f\"CACHED: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        print(\"NO GPU AVAILABLE\")\n",
    "        \n",
    "# PRINT INITIAL GPU MEMORY INFO\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\nINITIAL GPU MEMORY USAGE:\")\n",
    "    print(f\"ALLOCATED: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "    print(f\"CACHED: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db0dcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DATASETS...\n",
      "DATASETS LOADED SUCCESSFULLY.\n",
      "TRAINING SET: 148388 SAMPLES\n",
      "VALIDATION SET: 42396 SAMPLES\n",
      "TEST SET: 21199 SAMPLES\n",
      "\n",
      "CLASS DISTRIBUTION IN TRAINING SET:\n",
      "CLASS 1: 74196 SAMPLES (50.00%)\n",
      "CLASS 0: 74192 SAMPLES (50.00%)\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATASETS\n",
    "print(\"LOADING DATASETS...\")\n",
    "\n",
    "try:\n",
    "    # READ CSV FILES\n",
    "    train_df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "    val_df = pd.read_csv(VAL_CSV_PATH)\n",
    "    test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "    \n",
    "    print(\"DATASETS LOADED SUCCESSFULLY.\")\n",
    "    \n",
    "    # BASIC DATA EXPLORATION\n",
    "    print(f\"TRAINING SET: {train_df.shape[0]} SAMPLES\")\n",
    "    print(f\"VALIDATION SET: {val_df.shape[0]} SAMPLES\")\n",
    "    print(f\"TEST SET: {test_df.shape[0]} SAMPLES\")\n",
    "    \n",
    "    # CLASS BALANCE IN TRAINING SET\n",
    "    class_counts = train_df['Label'].value_counts()\n",
    "    print(\"\\nCLASS DISTRIBUTION IN TRAINING SET:\")\n",
    "    for label, count in class_counts.items():\n",
    "        print(f\"CLASS {label}: {count} SAMPLES ({count/len(train_df)*100:.2f}%)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR DURING LOADING: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d23bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZING BERT TOKENIZER...\n",
      "CREATING DATASETS...\n",
      "DATALOADERS CREATED WITH BATCH SIZE: 32\n",
      "CREATING DATASETS...\n",
      "DATALOADERS CREATED WITH BATCH SIZE: 32\n"
     ]
    }
   ],
   "source": [
    "# INITIALIZE BERT TOKENIZER\n",
    "print(\"INITIALIZING BERT TOKENIZER...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# DEFINE CUSTOM DATASET FOR SENTIMENT ANALYSIS WITH BERT\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_length=128, is_test=False):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.is_test = is_test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        \n",
    "        # TOKENIZE THE TEXT FOR BERT\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,  # ADDS [CLS] AND [SEP]\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # CONVERT TO FLAT TENSORS FROM BATCHED TENSORS\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "        \n",
    "        # ADD LABEL FOR TRAINING DATA\n",
    "        if not self.is_test and self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            \n",
    "        # ADD ID FOR TEST DATA\n",
    "        if self.is_test:\n",
    "            item['id'] = torch.tensor(idx)\n",
    "            \n",
    "        return item\n",
    "\n",
    "# CREATE DATASETS\n",
    "print(\"CREATING DATASETS...\")\n",
    "train_dataset = SentimentDataset(\n",
    "    train_df['Text'], \n",
    "    train_df['Label'].values, \n",
    "    tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "val_dataset = SentimentDataset(\n",
    "    val_df['Text'], \n",
    "    val_df['Label'].values, \n",
    "    tokenizer,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "test_dataset = SentimentDataset(\n",
    "    test_df['Text'], \n",
    "    labels=None,  \n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "# CREATE DATALOADERS\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"DATALOADERS CREATED WITH BATCH SIZE: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5715fa91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZING BERT MODEL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT MODEL INITIALIZED WITH LEARNING RATE: 2e-05\n",
      "TRAINING FOR 4 EPOCHS, 18552 TOTAL STEPS\n"
     ]
    }
   ],
   "source": [
    "# INITIALIZE BERT MODEL FOR SEQUENCE CLASSIFICATION\n",
    "print(\"INITIALIZING BERT MODEL...\")\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2  # BINARY CLASSIFICATION (0 OR 1)\n",
    ").to(DEVICE)\n",
    "\n",
    "# DEFINE OPTIMIZER WITH WEIGHT DECAY\n",
    "optimizer = AdamW(\n",
    "    bert_model.parameters(),\n",
    "    lr=BERT_LEARNING_RATE,\n",
    "    # PYTORCH'S ADAMW DOESN'T HAVE CORRECT_BIAS PARAMETER\n",
    "    weight_decay=0.01  # DEFAULT WEIGHT DECAY FOR BERT FINE-TUNING\n",
    ")\n",
    "\n",
    "# CALCULATE TRAINING STEPS AND SETUP SCHEDULER\n",
    "total_steps = len(train_loader) * BERT_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=WARMUP_STEPS,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"BERT MODEL INITIALIZED WITH LEARNING RATE: {BERT_LEARNING_RATE}\")\n",
    "print(f\"TRAINING FOR {BERT_EPOCHS} EPOCHS, {total_steps} TOTAL STEPS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e38a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT TRAINING FUNCTION\n",
    "def train_bert(model, dataloader, optimizer, scheduler, device):\n",
    "    \"\"\"\n",
    "    TRAINS THE BERT MODEL FOR ONE EPOCH WITH MIXED PRECISION IF AVAILABLE\n",
    "    \"\"\"\n",
    "    # SET MODEL TO TRAINING MODE\n",
    "    model.train()\n",
    "    \n",
    "    # TRACK METRICS\n",
    "    epoch_loss = 0\n",
    "    epoch_corrects = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # ADD PROGRESS BAR\n",
    "    for batch in tqdm(dataloader, desc=\"TRAINING\"):\n",
    "        # MOVE BATCH TO DEVICE\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # CLEAR GRADIENTS\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # USE MIXED PRECISION TRAINING IF AVAILABLE (FASTER ON T4 GPU)\n",
    "        if mixed_precision_available:\n",
    "            with autocast():\n",
    "                # FORWARD PASS - BERT MODELS RETURN A NAMEDTUPLE WITH MANY FIELDS\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                # EXTRACT LOSS AND LOGITS\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "            \n",
    "            # BACKWARD PASS WITH GRADIENT SCALING\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # CLIP GRADIENTS - PREVENTS EXPLODING GRADIENTS\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            # UPDATE WEIGHTS WITH SCALING\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            # STANDARD TRAINING WITHOUT MIXED PRECISION\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            # EXTRACT LOSS AND LOGITS\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # BACKWARD PASS\n",
    "            loss.backward()\n",
    "            \n",
    "            # CLIP GRADIENTS - PREVENTS EXPLODING GRADIENTS\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            # UPDATE WEIGHTS\n",
    "            optimizer.step()\n",
    "        \n",
    "        # UPDATE LEARNING RATE SCHEDULE\n",
    "        scheduler.step()\n",
    "        \n",
    "        # GET PREDICTIONS AND CALCULATE ACCURACY\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        corrects = (preds == labels).sum().item()\n",
    "        \n",
    "        # ACCUMULATE METRICS\n",
    "        batch_size = labels.size(0)\n",
    "        epoch_loss += loss.item() * batch_size\n",
    "        epoch_corrects += corrects\n",
    "        total_samples += batch_size\n",
    "    \n",
    "    # CALCULATE EPOCH METRICS\n",
    "    avg_loss = epoch_loss / total_samples\n",
    "    accuracy = epoch_corrects / total_samples\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# BERT EVALUATION FUNCTION\n",
    "def evaluate_bert(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    EVALUATES THE BERT MODEL ON A VALIDATION OR TEST SET\n",
    "    \"\"\"\n",
    "    # SET MODEL TO EVALUATION MODE\n",
    "    model.eval()\n",
    "    \n",
    "    # TRACK METRICS\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_samples = 0\n",
    "    \n",
    "    # NO GRADIENT CALCULATION DURING EVALUATION\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"EVALUATING\"):\n",
    "            # MOVE BATCH TO DEVICE\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # FORWARD PASS\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            # EXTRACT LOSS AND LOGITS\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # GET PREDICTIONS\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            labels_cpu = labels.cpu().numpy()\n",
    "            \n",
    "            # ACCUMULATE METRICS\n",
    "            batch_size = labels.size(0)\n",
    "            epoch_loss += loss.item() * batch_size\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels_cpu)\n",
    "            total_samples += batch_size\n",
    "    \n",
    "    # CALCULATE EPOCH METRICS\n",
    "    avg_loss = epoch_loss / total_samples\n",
    "    \n",
    "    # CALCULATE CLASSIFICATION METRICS\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='binary', zero_division=0\n",
    "    )\n",
    "    \n",
    "    return avg_loss, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d27fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING BERT FINE-TUNING...\n",
      "\n",
      "EPOCH 1/4\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# TRAINING PHASE\u001b[39;00m\n\u001b[1;32m     22\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 23\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_bert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbert_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m train_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# EVALUATION PHASE\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 41\u001b[0m, in \u001b[0;36mtrain_bert\u001b[0;34m(model, dataloader, optimizer, scheduler, device)\u001b[0m\n\u001b[1;32m     38\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# UPDATE WEIGHTS\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# UPDATE LEARNING RATE SCHEDULE\u001b[39;00m\n\u001b[1;32m     44\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Documents/University/Semester 8/AI2/Assignment3/.venv/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:124\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    123\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/University/Semester 8/AI2/Assignment3/.venv/lib/python3.9/site-packages/torch/optim/optimizer.py:485\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    482\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m             )\n\u001b[0;32m--> 485\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/University/Semester 8/AI2/Assignment3/.venv/lib/python3.9/site-packages/torch/optim/optimizer.py:79\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 79\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/Documents/University/Semester 8/AI2/Assignment3/.venv/lib/python3.9/site-packages/torch/optim/adam.py:246\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    234\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    236\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    237\u001b[0m         group,\n\u001b[1;32m    238\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m         state_steps,\n\u001b[1;32m    244\u001b[0m     )\n\u001b[0;32m--> 246\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoupled_weight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Documents/University/Semester 8/AI2/Assignment3/.venv/lib/python3.9/site-packages/torch/optim/optimizer.py:147\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/University/Semester 8/AI2/Assignment3/.venv/lib/python3.9/site-packages/torch/optim/adam.py:933\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    931\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 933\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/University/Semester 8/AI2/Assignment3/.venv/lib/python3.9/site-packages/torch/optim/adam.py:405\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoupled_weight_decay:\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;66;03m# Perform stepweight decay\u001b[39;00m\n\u001b[0;32m--> 405\u001b[0m         \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;66;03m# Nested if is necessary to bypass jitscript rules\u001b[39;00m\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m differentiable \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weight_decay, Tensor):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING LOOP FOR BERT\n",
    "# INITIALIZE LISTS TO TRACK METRICS\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "val_precisions = []\n",
    "val_recalls = []\n",
    "val_f1s = []\n",
    "\n",
    "# BEST MODEL TRACKING - USING VALIDATION LOSS INSTEAD OF F1\n",
    "best_val_loss = float('inf')  # INITIALIZE WITH INFINITY FOR LOSS (LOWER IS BETTER)\n",
    "BERT_MODEL_SAVE_PATH = os.path.join(OUTPUT_DIR, 'best_bert_model.pt')\n",
    "\n",
    "print(\"STARTING BERT FINE-TUNING...\")\n",
    "\n",
    "# IF AVAILABLE MEMORY IS LOW, CLEAR IT BEFORE TRAINING\n",
    "if torch.cuda.is_available() and torch.cuda.memory_allocated(0) > 1e9:\n",
    "    clear_gpu_memory()\n",
    "\n",
    "for epoch in range(BERT_EPOCHS):\n",
    "    print(f\"\\nEPOCH {epoch+1}/{BERT_EPOCHS}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # TRAINING PHASE\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_bert(bert_model, train_loader, optimizer, scheduler, DEVICE)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # EVALUATION PHASE\n",
    "    start_time = time.time()\n",
    "    val_loss, val_acc, val_precision, val_recall, val_f1 = evaluate_bert(bert_model, val_loader, DEVICE)\n",
    "    eval_time = time.time() - start_time\n",
    "    \n",
    "    # SAVE METRICS\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    val_precisions.append(val_precision)\n",
    "    val_recalls.append(val_recall)\n",
    "    val_f1s.append(val_f1)\n",
    "    \n",
    "    # SAVE BEST MODEL BASED ON VALIDATION LOSS (LOWER IS BETTER)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(bert_model.state_dict(), BERT_MODEL_SAVE_PATH)\n",
    "        print(f\"NEW BEST VALIDATION LOSS: {val_loss:.4f} - SAVED MODEL TO {BERT_MODEL_SAVE_PATH}\")\n",
    "    \n",
    "    # PRINT METRICS\n",
    "    print(f\"TRAIN - Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}, Time: {train_time:.2f}s\")\n",
    "    print(f\"EVAL  - Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}, Time: {eval_time:.2f}s\")\n",
    "    print(f\"EVAL  - Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n",
    "    \n",
    "    # Clear cache between epochs if using GPU\n",
    "    if torch.cuda.is_available():\n",
    "        clear_gpu_memory()\n",
    "\n",
    "print(\"\\nBERT FINE-TUNING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509fae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTION AND SUBMISSION GENERATION FOR BERT\n",
    "\n",
    "def predict_test_bert(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    GENERATE PREDICTIONS ON THE TEST SET USING THE FINE-TUNED BERT MODEL\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_ids = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(tqdm(dataloader, desc=\"PREDICTING\")):\n",
    "            # GET IDS FROM THE TEST DATAFRAME\n",
    "            start_idx = idx * BATCH_SIZE\n",
    "            end_idx = min(start_idx + BATCH_SIZE, len(test_df))\n",
    "            batch_ids = test_df['ID'].iloc[start_idx:end_idx].values\n",
    "            \n",
    "            # MOVE BATCH TO DEVICE\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # FORWARD PASS\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            # GET PREDICTIONS\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            \n",
    "            # STORE PREDICTIONS AND IDS\n",
    "            all_preds.extend(preds)\n",
    "            all_ids.extend(batch_ids)\n",
    "    \n",
    "    return all_ids, all_preds\n",
    "\n",
    "# LOAD THE BEST SAVED BERT MODEL\n",
    "print(f\"LOADING BEST BERT MODEL FROM {BERT_MODEL_SAVE_PATH}...\")\n",
    "bert_model.load_state_dict(torch.load(BERT_MODEL_SAVE_PATH))\n",
    "bert_model = bert_model.to(DEVICE)\n",
    "\n",
    "# GENERATE PREDICTIONS\n",
    "print(\"GENERATING PREDICTIONS ON TEST SET...\")\n",
    "test_ids, test_preds = predict_test_bert(bert_model, test_loader, DEVICE)\n",
    "\n",
    "# CREATE SUBMISSION DATAFRAME\n",
    "submission_df = pd.DataFrame({\n",
    "    'ID': test_ids,\n",
    "    'Label': test_preds\n",
    "})\n",
    "\n",
    "# SAVE SUBMISSION FILE TO KAGGLE WORKING DIRECTORY\n",
    "BERT_SUBMISSION_PATH = os.path.join(OUTPUT_DIR, 'bert_submission.csv')\n",
    "submission_df.to_csv(BERT_SUBMISSION_PATH, index=False)\n",
    "\n",
    "print(f\"BERT SUBMISSION SAVED TO {BERT_SUBMISSION_PATH}\")\n",
    "print(\"\\nFIRST FEW ROWS OF SUBMISSION:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe928fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION OF BERT MODEL PERFORMANCE\n",
    "\n",
    "# PLOT LEARNING CURVES\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# LOSS CURVE\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, BERT_EPOCHS+1), train_losses, 'b-', label='TRAINING LOSS')\n",
    "plt.plot(range(1, BERT_EPOCHS+1), val_losses, 'r-', label='VALIDATION LOSS')\n",
    "plt.xlabel('EPOCH')\n",
    "plt.ylabel('LOSS')\n",
    "plt.title('BERT LOSS CURVES')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# ACCURACY CURVE\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, BERT_EPOCHS+1), train_accs, 'b-', label='TRAINING ACCURACY')\n",
    "plt.plot(range(1, BERT_EPOCHS+1), val_accs, 'r-', label='VALIDATION ACCURACY')\n",
    "plt.xlabel('EPOCH')\n",
    "plt.ylabel('ACCURACY')\n",
    "plt.title('BERT ACCURACY CURVES')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'bert_learning_curves.png'))\n",
    "plt.show()\n",
    "\n",
    "# PLOT F1, PRECISION, RECALL\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, BERT_EPOCHS+1), val_precisions, 'g-', label='PRECISION')\n",
    "plt.plot(range(1, BERT_EPOCHS+1), val_recalls, 'b-', label='RECALL')\n",
    "plt.plot(range(1, BERT_EPOCHS+1), val_f1s, 'r-', label='F1 SCORE')\n",
    "plt.xlabel('EPOCH')\n",
    "plt.ylabel('SCORE')\n",
    "plt.title('BERT PERFORMANCE METRICS')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'bert_performance_metrics.png'))\n",
    "plt.show()\n",
    "\n",
    "# COMPUTE PREDICTIONS FOR CONFUSION MATRIX AND ROC CURVE\n",
    "print(\"COMPUTING FINAL VALIDATION METRICS...\")\n",
    "bert_model.eval()\n",
    "all_labels = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"COMPUTING METRICS\"):\n",
    "        # MOVE BATCH TO DEVICE\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "        \n",
    "        # FORWARD PASS\n",
    "        outputs = bert_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # GET PROBABILITIES FOR POSITIVE CLASS (CLASS 1)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "        \n",
    "        all_labels.extend(labels)\n",
    "        all_probs.extend(probs)\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "all_probs = np.array(all_probs)\n",
    "all_preds = (all_probs >= 0.5).astype(int)\n",
    "\n",
    "# CONFUSION MATRIX\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['NEGATIVE (0)', 'POSITIVE (1)'],\n",
    "            yticklabels=['NEGATIVE (0)', 'POSITIVE (1)'])\n",
    "plt.xlabel('PREDICTED LABEL')\n",
    "plt.ylabel('TRUE LABEL')\n",
    "plt.title('BERT CONFUSION MATRIX')\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'bert_confusion_matrix.png'))\n",
    "plt.show()\n",
    "\n",
    "# ROC CURVE\n",
    "fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, 'b-', label=f'ROC CURVE (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('FALSE POSITIVE RATE')\n",
    "plt.ylabel('TRUE POSITIVE RATE')\n",
    "plt.title('BERT ROC CURVE')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'bert_roc_curve.png'))\n",
    "plt.show()\n",
    "\n",
    "# PRINT FINAL METRICS\n",
    "print(f\"\\nBERT FINAL VALIDATION METRICS:\")\n",
    "print(f\"ACCURACY: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "print(f\"PRECISION: {val_precisions[-1]:.4f}\")\n",
    "print(f\"RECALL: {val_recalls[-1]:.4f}\")\n",
    "print(f\"F1 SCORE: {val_f1s[-1]:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# SAVE METRICS FOR FUTURE COMPARISON\n",
    "try:\n",
    "    np.savez(os.path.join(OUTPUT_DIR, 'bert_metrics.npz'), \n",
    "             train_losses=train_losses, \n",
    "             val_losses=val_losses,\n",
    "             train_accs=train_accs, \n",
    "             val_accs=val_accs,\n",
    "             val_precisions=val_precisions,\n",
    "             val_recalls=val_recalls,\n",
    "             val_f1s=val_f1s)\n",
    "    print(\"\\nBERT metrics saved for future comparison with other models.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving metrics: {e}\")\n",
    "\n",
    "# SAVE A SUMMARY OF THE RESULTS IN A TEXT FILE\n",
    "with open(os.path.join(OUTPUT_DIR, 'bert_results_summary.txt'), 'w') as f:\n",
    "    f.write(\"BERT SENTIMENT CLASSIFICATION RESULTS\\n\")\n",
    "    f.write(\"===================================\\n\\n\")\n",
    "    f.write(f\"Best Validation Loss: {best_val_loss:.4f}\\n\")\n",
    "    f.write(f\"Final Accuracy: {accuracy_score(all_labels, all_preds):.4f}\\n\")\n",
    "    f.write(f\"Final Precision: {val_precisions[-1]:.4f}\\n\")\n",
    "    f.write(f\"Final Recall: {val_recalls[-1]:.4f}\\n\")\n",
    "    f.write(f\"Final F1 Score: {val_f1s[-1]:.4f}\\n\")\n",
    "    f.write(f\"ROC AUC: {roc_auc:.4f}\\n\\n\")\n",
    "    f.write(\"Training completed successfully.\\n\")\n",
    "    f.write(f\"Submission file saved to: {BERT_SUBMISSION_PATH}\\n\")\n",
    "\n",
    "print(\"\\nResults summary saved to:\", os.path.join(OUTPUT_DIR, 'bert_results_summary.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e00425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU MEMORY MANAGEMENT\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"CLEAR GPU MEMORY CACHE TO FREE UP RESOURCES.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"GPU MEMORY CACHE CLEARED\")\n",
    "        # PRINT CURRENT MEMORY USAGE\n",
    "        print(f\"GPU MEMORY: ALLOCATED: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB, \"\n",
    "              f\"CACHED: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "    elif hasattr(torch, 'mps') and torch.backends.mps.is_available():\n",
    "        # MPS DOESN'T HAVE EXPLICIT MEMORY MANAGEMENT FUNCTIONS LIKE CUDA\n",
    "        print(\"NOTE: MPS (APPLE SILICON) DOESN'T SUPPORT EXPLICIT MEMORY CLEARING\")\n",
    "    else:\n",
    "        print(\"NO GPU AVAILABLE\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
